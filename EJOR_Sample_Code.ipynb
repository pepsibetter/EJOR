{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pepsibetter/EJOR/blob/main/EJOR_Sample_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgIIHDsK9eIx"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O6SDYBU-sJF"
      },
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import collections\n",
        "import heapq\n",
        "import copy\n",
        "import math\n",
        "import queue\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch.nn import Linear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUimDlt_d7il"
      },
      "source": [
        "def generate_pages(num_pages):\n",
        "    adjacency = collections.defaultdict(set) # dictionary to store links of certain pages\n",
        "    vertices = num_pages # number of pages\n",
        "    outdegree = collections.defaultdict(int)\n",
        "    indegree = collections.defaultdict(int)\n",
        "    size = dict() # to store size of each page\n",
        "    a = 1.72\n",
        "    m = 1\n",
        "    \n",
        "    for i in range(vertices): # page id 0 to (vertices - 1)\n",
        "        out_degree = math.ceil((np.random.pareto(a,) + 1)*m)\n",
        "        outdegree[i] = out_degree\n",
        "        \n",
        "        while(out_degree):\n",
        "            link = random.randint(0, vertices - 1)\n",
        "            if link != i and link not in adjacency[i]:\n",
        "                adjacency[i].add(link)\n",
        "                out_degree -= 1\n",
        "    \n",
        "    for v in adjacency.keys():\n",
        "        for n in adjacency[v]:\n",
        "            indegree[n] += 1\n",
        "        size[v] = abs(random.gauss(10, 6)) # randomly assign size amount to each page\n",
        "    \n",
        "    homepage = 0\n",
        "\n",
        "    return homepage, adjacency, size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lfoVvJVd_IG"
      },
      "source": [
        "def Dijkstra(source):\n",
        "    global size\n",
        "    global adjacency\n",
        "    \n",
        "    linkedpages = set() # to store visited ones\n",
        "    queue = [source]\n",
        "    while(queue):\n",
        "        page = queue.pop()\n",
        "        linkedpages.add(page)\n",
        "        for p in adjacency[page]:\n",
        "            if p not in linkedpages:\n",
        "                queue.append(p)\n",
        "    \n",
        "    dist = {}\n",
        "    for page_id in linkedpages:\n",
        "        dist[page_id] = [float('inf'), str()] # slower when adding path, maybe caused by list structure\n",
        "    dist[source] = [0, str(source)]\n",
        "    Q = [] # a priority queue\n",
        "    S = set() # to store already visited page\n",
        "    heapq.heappush(Q, (dist[source][0], source, dist[source][1]))\n",
        "    N = len(linkedpages)\n",
        "    \n",
        "    while(len(S) < N):\n",
        "        dis, page_id, path = heapq.heappop(Q)\n",
        "        if page_id not in S:\n",
        "            S.add(page_id)\n",
        "            for next_page in adjacency[page_id]:\n",
        "                if next_page not in S:\n",
        "                    #dist[next_page][0] = min(dist[next_page][0], dis + size[next_page])\n",
        "                    if dis + size[next_page] < dist[next_page][0]:\n",
        "                        dist[next_page][0] = dis + size[next_page]\n",
        "                        dist[next_page][1] = path + '-' + str(next_page)\n",
        "                    heapq.heappush(Q, (dist[next_page][0], next_page, dist[next_page][1]))        \n",
        "    \n",
        "    return dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czSZ13DmeB3y"
      },
      "source": [
        "def subsets(terminals):\n",
        "    allsubsets = []\n",
        "    print(len(terminals))\n",
        "    for i in range(int(math.pow(2, len(terminals)))):\n",
        "        subset = []\n",
        "        \n",
        "        for j in range(len(terminals)):\n",
        "            if (i&(1 << j) > 0):\n",
        "                subset.append(terminals[j])\n",
        "        \n",
        "        allsubsets.append(subset)\n",
        "    \n",
        "    return allsubsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPFsKYm5eEi7"
      },
      "source": [
        "def Transition(S):\n",
        "    visited = set()\n",
        "    while(Q):\n",
        "        dis, node = heapq.heappop(Q)\n",
        "#         if node not in visited:\n",
        "#             visited.add(node)\n",
        "        for last_node in indegree[node]:\n",
        "            if dp[node][S] + size[node] < dp[last_node][S]:\n",
        "                tmp = set([node])\n",
        "                for j in path[node][S]:\n",
        "                     tmp.add(j)\n",
        "                path[last_node][S] = tmp.copy()\n",
        "                dp[last_node][S] = dp[node][S] +size[node]\n",
        "                heapq.heappush(Q, (dp[last_node][S], last_node))\n",
        "#             for next_node in adjacency[node]:\n",
        "#                 if dp[next_node][S] + size[next_node] < dp[node][S]:\n",
        "#                     dp[node][S] =  dp[next_node][S] + size[next_node]\n",
        "#                     heapq.heappush(Q, (dp[node][S], node))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZg3SLyzeH_p"
      },
      "source": [
        "homepage, adjacency, size = generate_pages(100) # the scale is 100\n",
        "k = 3\n",
        "dist = Dijkstra(homepage)\n",
        "indegree = collections.defaultdict(set)\n",
        "for i in adjacency.keys():\n",
        "    for j in adjacency[i]:\n",
        "        indegree[j].add(i)\n",
        "\n",
        "dest_set = []\n",
        "result_set = []\n",
        "\n",
        "num_samples = 40000\n",
        "\n",
        "for _ in range(num_samples): # generate data \n",
        "    destinations = random.sample(dist.keys(), k) \n",
        "    while homepage in destinations:\n",
        "        destinations = random.sample(dist.keys(), k)\n",
        "    destinations = sorted(destinations)\n",
        "\n",
        "    # initalize\n",
        "    endstate = 1<<k\n",
        "    dp = [[float('inf') for i in range(endstate)] for _ in range(100)]\n",
        "    count = 0\n",
        "    Q = []\n",
        "    for i in range(100):\n",
        "        if i in destinations:\n",
        "            dp[i][1<<count] = 0\n",
        "            count += 1\n",
        "\n",
        "    path = [[set([i]) for _ in range(endstate)] for i in range(100)]\n",
        "\n",
        "    # DP\n",
        "    for S in range(1, 1<<k):\n",
        "        for i in range(100):\n",
        "            sub = (S-1)&S\n",
        "            while(sub):\n",
        "                if dp[i][sub]+dp[i][S^sub] < dp[i][S]:\n",
        "                    dp[i][S] = dp[i][sub]+dp[i][S^sub]\n",
        "                    tmp = set()\n",
        "                    for j in path[i][sub]:\n",
        "                        tmp.add(j)\n",
        "                    for j in path[i][S^sub]:\n",
        "                        tmp.add(j)\n",
        "                    path[i][S] = tmp.copy()\n",
        "                sub = (sub-1)&S\n",
        "\n",
        "            if dp[i][S] < float('inf'):\n",
        "                heapq.heappush(Q, (dp[i][S], i))\n",
        "\n",
        "        Transition(S)\n",
        "\n",
        "    result = dp[homepage][(1<<k)-1]\n",
        "    guidance = path[homepage][-1]\n",
        "    #print(guidance)\n",
        "    result_set.append(guidance)\n",
        "    dest_set.append(destinations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6iAvrM_eg_Y"
      },
      "source": [
        "node_feature = [] # Node-weighted Steiner Tree\n",
        "#min_size = np.min(list(size.values())) # feature normalization\n",
        "#max_size = np.max(list(size.values()))\n",
        "for i in range(len(dest_set)):\n",
        "  features = [[x, 0] for x in size.values()]\n",
        "  features[0][1] = 1\n",
        "  for j in dest_set[i]:\n",
        "    features[j][1] = 1\n",
        "  node_feature.append(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40D2zKXKo4jU"
      },
      "source": [
        "edges = []\n",
        "for i in adjacency.keys():\n",
        "  for j in adjacency[i]:\n",
        "    edges.append([i, j])\n",
        "edge_index = torch.tensor(edges, dtype=torch.long) # Edge connectivity with shape [2, num_edges]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9paxPBgbEb-q"
      },
      "source": [
        "labels = [] # target labels\n",
        "for i in range(len(result_set)):\n",
        "  plain = [[0] for _ in range(100)]\n",
        "  plain[0] = [1]\n",
        "  for j in result_set[i]:\n",
        "    plain[j] = [1]\n",
        "  \n",
        "  labels.append(plain)\n",
        "\n",
        "#y = torch.tensor(labels[0], dtype=torch.int64) # targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G6ZtH8iwh3e"
      },
      "source": [
        "train_dataset = []\n",
        "for i in range(int(num_samples*0.7)):\n",
        "  x = torch.tensor(node_feature[i], dtype=torch.float) # Node feature matrix with shape [num_nodes, num_node_features (2 here, size, flag)]\n",
        "  y = torch.tensor(labels[i], dtype=torch.float) # targets\n",
        "  data = Data(x=x, y=y, edge_index=edge_index.t().contiguous()) # one set\n",
        "  train_dataset.append(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G_jQy-yT5dM"
      },
      "source": [
        "test_dataset = []\n",
        "for i in range(int(num_samples*0.7), num_samples):\n",
        "  x = torch.tensor(node_feature[i], dtype=torch.float) # Node feature matrix with shape [num_nodes, num_node_features (2 here, size, flag)]\n",
        "  y = torch.tensor(labels[i], dtype=torch.float) # targets\n",
        "  data = Data(x=x, y=y, edge_index=edge_index.t().contiguous()) # one set\n",
        "  test_dataset.append(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GifhPOIgUfvY"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWfV2k6I9qw3"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = GATConv(2, 256, heads=4) # from input feature dimension\n",
        "        self.lin1 = torch.nn.Linear(2, 4 * 256)\n",
        "        \n",
        "        self.conv2 = GATConv(4 * 256, 256, heads=4)\n",
        "        self.lin2 = torch.nn.Linear(4 * 256, 4 * 256)\n",
        "        \n",
        "        self.conv3 = GATConv(4 * 256, 256, heads=4)\n",
        "        self.lin3 = torch.nn.Linear(4 * 256, 4 * 256)\n",
        "        \n",
        "        self.conv4 = GATConv(4 * 256, 256, heads=4)\n",
        "        self.lin4 = torch.nn.Linear(4 * 256, 4 * 256)\n",
        "        \n",
        "        self.conv5 = GATConv(4 * 256, 256, heads=4)\n",
        "        self.lin5 = torch.nn.Linear(4 * 256, 4 * 256)\n",
        "\n",
        "        self.conv6 = GATConv(4 * 256, 256, heads=4)\n",
        "        self.lin6 = torch.nn.Linear(4 * 256, 4 * 256)\n",
        "\n",
        "        self.conv7 = GATConv(4 * 256, 256, heads=4)\n",
        "        self.lin7 = torch.nn.Linear(4 * 256, 4 * 256)\n",
        "\n",
        "        self.conv8 = GATConv(4 * 256, 256, heads=4)\n",
        "        self.lin8 = torch.nn.Linear(4 * 256, 4 * 256)\n",
        "\n",
        "        self.conv9 = GATConv(4 * 256, 256, heads=4)\n",
        "        self.lin9 = torch.nn.Linear(4 * 256, 4 * 256)\n",
        "\n",
        "        self.conv10 = GATConv(4 * 256, 256, heads=4)\n",
        "        self.lin10 = torch.nn.Linear(4 * 256, 4 * 256)\n",
        "\n",
        "        self.conv11 = GATConv(4 * 256, 256, heads=4)\n",
        "        self.lin11 = torch.nn.Linear(4 * 256, 4 * 256)\n",
        "\n",
        "        self.conv12 = GATConv(4 * 256, 1, heads=6,\n",
        "                             concat=False)\n",
        "        self.lin12 = torch.nn.Linear(4 * 256, 1) # to output class\n",
        "\n",
        "        self.norm1 = torch.nn.BatchNorm1d(4*256)\n",
        "        self.norm2 = torch.nn.BatchNorm1d(4*256)\n",
        "        self.norm3 = torch.nn.BatchNorm1d(4*256)\n",
        "        self.norm4 = torch.nn.BatchNorm1d(4*256)\n",
        "        self.norm5 = torch.nn.BatchNorm1d(4*256)\n",
        "        self.norm6 = torch.nn.BatchNorm1d(4*256)\n",
        "        self.norm7 = torch.nn.BatchNorm1d(4*256)\n",
        "        self.norm8 = torch.nn.BatchNorm1d(4*256)\n",
        "        self.norm9 = torch.nn.BatchNorm1d(4*256)\n",
        "        self.norm10 = torch.nn.BatchNorm1d(4*256)\n",
        "        self.norm11 = torch.nn.BatchNorm1d(4*256)\n",
        "\n",
        "        # self.convs = torch.nn.ModuleList()\n",
        "        # for layer in range(11):\n",
        "        #     self.convs.append(GATConv(4 * 256, 256, heads=4))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index) + self.lin1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.conv2(x, edge_index) + self.lin2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.conv3(x, edge_index) + self.lin3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.conv4(x, edge_index) + self.lin4(x)\n",
        "        x = self.norm4(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv5(x, edge_index) + self.lin5(x)\n",
        "        x = self.norm5(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.conv6(x, edge_index) + self.lin6(x)\n",
        "        x = self.norm6(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv7(x, edge_index) + self.lin7(x)\n",
        "        x = self.norm7(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv8(x, edge_index) + self.lin8(x)\n",
        "        x = self.norm8(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv9(x, edge_index) + self.lin9(x)\n",
        "        x = self.norm9(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv10(x, edge_index) + self.lin10(x)\n",
        "        x = self.norm10(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv11(x, edge_index) + self.lin11(x)\n",
        "        x = self.norm11(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv12(x, edge_index) + self.lin12(x)\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba0DhECaU1ni"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01) # Or use Adadelta "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJQEFLctXiVB"
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(data.x, data.edge_index), data.y)\n",
        "        total_loss += loss.item() * data.num_graphs        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(train_loader.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "280CZATpbOF6"
      },
      "source": [
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    #global check_1, check_2\n",
        "    model.eval()\n",
        "\n",
        "    ys, preds = [], []\n",
        "    count = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x.to(device), data.edge_index.to(device))\n",
        "        # check_1.append(data.y)\n",
        "        # check_2.append(out)\n",
        "        predicted = (out > 0.5).float().cpu()\n",
        "        \n",
        "        target_y = data.y.numpy().tolist()\n",
        "        pred_y = predicted.numpy().tolist()\n",
        "\n",
        "        ys.append(data.y)\n",
        "        preds.append((out > 0.5).float().cpu())\n",
        "\n",
        "        flag = 1\n",
        "        for i in range(len(target_y)):\n",
        "          if pred_y[i] != target_y[i]:\n",
        "            flag = 0\n",
        "        \n",
        "        if flag:\n",
        "          count += 1\n",
        "    \n",
        "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()    \n",
        "    node_count = 0\n",
        "    for j in range(len(y)):\n",
        "      if y[j] == pred[j]:\n",
        "        node_count += 1\n",
        "\n",
        "    return count / len(test_loader.dataset), node_count / len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    loss = train()\n",
        "    train_accuracy, train_node = test(train_loader)\n",
        "    test_accuracy, test_node = test(test_loader)\n",
        "    print('Epoch: {:02d}, Loss: {:.4f}, Node accuracy of train: {:.4f}, Node accuracy of test: {:.4f}, Path accuracy of Test: {:.4f}'.format(\n",
        "        epoch, loss, train_node, test_node, test_accuracy))"
      ],
      "metadata": {
        "id": "oR0Bzc0Wz5t4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}